# GenBench: The first workshop on generalisation (benchmarking) in NLP

## Workshop description
The ability to generalise well is often mentioned as one of the primary desiderata for models of natural language processing. It is crucial to ensure that models behave robustly, reliably and fairly when making predictions about data that is different from the data that they were trained on. Generalisation is also important when NLP models are considered from a cognitive perspective, as models of human language. Yet, there are still many open questions related to what it means for an NLP model to generalise well, and how generalisation should be evaluated. The first GenBench workshop aims to serve as a cornerstone to catalyse research on generalisation in the NLP community. In particular the workshop aims to:
Bring together different expert communities to discuss challenging questions relating to generalisation in NLP;
Crowd-source a collaborative generalisation benchmark, hosted on a platform for democratic state-of-the-art (SOTA) generalisation testing in NLP

The first GenBench workshop on generalisation (benchmarking) in NLP will be co-located with EMNLP 2023.

## Submission types
We call for two types of submissions: regular workshop submissions and collaborative benchmarking task submissions. The latter will consist of a data/task artefact and a companion paper motivating and evaluating the submission. In both cases, we accept archival papers and extended abstracts. 

###  Regular workshop submissions
Regular workshop submissions present papers on the topic of generalisation (see examples listed below), but are not intended to be included on the GenBench evaluation platform. Regular workshop papers may be submitted as an archival paper, when they report on completed, original and unpublished research; or as a shorter extended abstract. More details on this category can be found below.

Topics of interest include, but are not limited to:
- Opinion or position papers about generalisation and how it should be evaluated;
- Analyses of how existing or new models generalise;
- Empirical studies that propose new paradigms to evaluate generalisation;
- Meta analyses that compare how results from different generalisation studies compare;
- Meta analyses that study how different types of generalisation are related;
- Papers that discuss how generalisation of LLMs can be evaluated without access to training data;
- Papers that discuss why generalisation is (not) important in the era of LLMs.
- Studies on the relationship between generalisation and fairness or robustness;

If you are unsure whether a specific topic is well-suited for submission, feel free to reach out to the organisers of the workshop at genbench@googlegroups.com.

### Collaborative Benchmarking Task submissions
Collaborative benchmarking task submissions consist of a data/task artefact and a paper describing and motivating the submission and showcasing it on a select number of models. 

We accept submissions that introduce new datasets, resplits of existing datasets along particular dimensions, or in-context learning tasks, with the goal of measuring generalisation of NLP models. We especially encourage submissions that focus on:
- Generalisation in the context of fairness and inclusivity
- Multilingual generalisation
- Generalisation in LLMs, where we have no control over the training data

Each submission should contain information about the data (URIs, format, preprocessing), model preparation (finetuning loss, ICL prompt templates), and evaluation metrics. These will be defined either in a configuration file or in code. More details about the collaborative benchmark submissions and example submissions can be found on our website: genbench.org/cbt.

Participants proposing previously unpublished datasets or splits may choose to submit an archival paper or an extended abstract. Generalisation evaluation datasets that have already been published elsewhere (or will be published at EMNLP 2023) can be submitted to the platform, as well, but only through an extended abstract, citing the original publication. We allow dual submissions with EMNLP, for more information, see below.

If you are in doubt whether a particular type of dataset is suitable for submission, please consult the information page on our website, or reach out to the organisers of the workshop at genbench@googlegroups.com.

## Archival vs extended abstract
Archival papers are up to 8 pages excluding references and report on completed, original and unpublished research. They follow the requirements of regular EMNLP 2023 submissions. Accepted papers will be published in the workshop proceedings and are expected to be presented at the workshop. The papers will undergo double-blind peer-review and should thus be anonymised. Extended abstracts can be up to 2 pages excluding references, and may report on work in progress or be cross submissions of work that has already appeared in another venue. Abstract titles will be posted on the workshop website, but will not be included in the proceedings.

## Submission instructions
For both archival papers and extended abstracts, we refer to the EMNLP 2023 website for paper templates. Additional requirements for both regular workshop papers and collaborative benchmarking task submissions can be found on our website. 

All submissions can be submitted through OpenReview, the link can be found on our website.

We also accept regular workshop submissions (papers of category 1) through the ACL Rolling Review system. Authors that have their ARR reviews ready may submit their papers and reviews for consideration to the workshop up to two weeks before our notification deadline.

## Important dates

August 1, 2023 â€“ Sample data submission deadline
September 1, 2023 â€“ Paper submission deadline
September 15, 2023 â€“ ARR submission deadline
October 6, 2023 â€“ Notification deadline
October 18, 2023 â€“ Camera ready deadline
December 6/7, 2023 â€“ Workshop

Note: all deadlines are 11:59PM UTC-12:00

## Dual submissions
We allow dual submissions with EMNLP, and we encourage relevant papers that were dual-submitted and accepted at EMNLP to redirect to a non-archival extended abstract submission. We furthermore welcome submissions of extended abstracts that describe work already presented at an earlier venue, both in the collaborative benchmarking and in the regular submission tracks.

## Preprints
We do not have an anonymity deadline, preprints are allowed, both before the submission deadline as well as after.

## Contact
Email address: genbench@googlegroups.com
Website: genbench.org/workshop






Mini CfPs for Twitter
XL
The 1st GenBench workshop (genbench.org/workshop) is calling for work on generalisation! Submit your paper to the regular track, or submit your data + paper to our ðŸ’¥collaborative benchmarking task (CBT)ðŸ’¥ before September 1. Will we see you at #EMNLP2023? 1/7

Generalisation is THE capability we want models to have. But what does it mean to generalise well? How do we evaluate that? ðŸ¤” Through the workshop, we discuss these questions and launch a platform for democratic SOTA generalisation testing in NLP. 2/7

(1) Submit your generalisation paper to the ðŸŽ“regular trackðŸŽ“ if you do not propose a task for the platform but, e.g. have an opinion piece, a generalisation-inspired analysis, meta-research about how NLP articles talk about generalisation etc. etc. 3/7

(2) Submit your task and data to the ðŸ’¥collaborative benchmarking task (CBT)ðŸ’¥ if your task evaluates non-iid generalisation in a novel manner, e.g. by resplitting existing datasets, proposing a new dataset, or a new in-context learning paradigm etc. etc. 4/7

Submitting to the ðŸ’¥CBTðŸ’¥ involves a paper submission AND a data submission to populate the GenBenchmark with accepted submissions. Visit genbench.org/cbt for special instructions on data submission formats. 5/7

For papers, the deadline is September 1, 2023, via Openreview. (For CBT *only*, a data-only pre-submission is required by August 1, 2023). GenBench accepts archival papers and extended abstracts for both tracks! Dual submissions and public preprints are allowed. 6/7

Are you intrigued? You should be! The era of SOTA generalisation evaluation is here ðŸ”¥, and we are talking about it at our workshop with @<...>, @<...>, @<...> at December 6, 2023. Visit genbench.org/workshop for all the details. 7/7
S
X days until the 1st GenBench workshop (genbench.org/workshop) at #EMNLP2023! Y days until the submission deadline on September 1. Submit your paper to the regular track, or submit your data + paper to our collaborative benchmarking task (CBT). A quick recap of the CfP: 1/3

At the workshop, we discuss what SOTA generalisation means and how to evaluate it! With YOUR data submissions, we launch a platform for NLP generalisation evaluation. You canâ€¦ (1) submit your data + paper to the Collaborative Benchmarking Task (see genbench.org/cbt) 2/3

Orâ€¦ (2) submit other papers about generalisation to the regular track. The paper deadlines are Sept 1, and CBT has a pre-submission deadline Aug 1. GenBench accepts archival papers and extended abstracts! Will we see you in Singapore to talk about all things generalisation?ðŸ”¥3/3

XS
Are you working on non-iid generalisation? Consider submitting to the GenBench workshop (genbench.org/workshop) #EMNLP2023. Submit your paper to the regular track or your dataset that evaluates generalisation in a novel way to our Collaborative Benchmarking Task (CBT)! 1/2

With YOUR CBT submissions, we launch a platform for NLP generalisation evaluation (see genbench.org/cbt). Submit your work as an archival paper or extended abstract (deadline September 1). Resubmissions of existing datasets are welcome! See you in Singapore!ðŸ”¥2/2

Emailing CfP: templates
Generic CfP send-off
Subject = Call for Papers: GenBench, the first workshop on benchmarking generalisation in NLP

Dear colleagues,

We invite you to submit to the GenBench workshop, the first workshop on benchmarking generalisation in NLP, co-located with EMNLP 2023.
Generalisation is vital for NLP models, but how do we evaluate that beyond the standard i.i.d testing paradigms?
The first GenBench workshop (1) facilitates discussion on research on generalisation in the NLP community and (2) crowdsources a collaborative generalisation benchmark.

We call for two types of submissions:
Regular workshop submissions that present papers on the topic of generalisation but are not intended to be included on the GenBench evaluation platform.
For example: opinion papers about how generalisation should be evaluated, analyses of how existing models generalise, or meta-analyses that compare how results from different generalisation studies compare.
Collaborative Benchmarking Task (CBT) submissions that consist of a data/task artefact and a paper describing the submission.
We accept submissions introducing new datasets, new splits of existing datasets, or in-context learning tasks, as long as the submission aims to evaluate non-i.i.d. generalisation of NLP models.
Note that a data sample pre-submission is required. See genbench.org/cbt for further details.

For both submission types, you can submit archival papers and extended abstracts.
Generalisation evaluation datasets that have been published elsewhere (or will be published at EMNLP 2023) can be submitted to the CBT, as well, if accompanied by an extended abstract. 

Dual submissions with EMNLP, non-anonymous preprints, and ARR submissions are welcome.
Further information about the workshop can be found in the full Call for Papers below or on our website. You can reach us for questions via genbench@googlegroups.com.

Key deadlines:
August 1, 2023 â€“ Sample data submission deadline
September 1, 2023 â€“ Paper submission deadline
September 15, 2023 â€“ ARR submission deadline

On behalf of the GenBench team,
Dieuwke Hupkes
â€‹Khuyagbaatar Batsuren
Koustuv Sinha
Amirhossein Kazemnejad
Christos Christodoulopoulos
Ryan Cotterell
Elia Brunâ€‹iâ€‹
â€‹Verna Dankers


===================================================


<insert CfP>
Outreach to other fields

Dear colleague,

We are organising the GenBench workshop (the first workshop on benchmarking generalisation in NLP), co-located with EMNLP 2023.
Generalisation is vital for NLP models, but how do we evaluate that beyond the standard i.i.d testing paradigms?

The first GenBench workshop (1) facilitates discussion on research on generalisation in the NLP community and (2) crowdsources a collaborative generalisation benchmark.
We call for two types of submissions:
1) Regular workshop submissions that present papers on the topic of generalisation but are not intended to be included in the collaborative benchmark.
2) Collaborative Benchmarking Task (CBT) submissions that consist of a data/task artefact and a paper describing the submission.

We specifically want to draw your attention to submission type 1. 
Based on your expertise, you may have opinions about how the NLP community, as a whole, does or does not yet handle the evaluation of realistic generalisation capabilities, or you may have ideas that could help NLP researchers to improve generalisation evaluation in the future.
We'd like to hear from you: consider submitting your thoughts via an archival paper or extended abstract as a GenBench workshop submission.
Further information about the workshop can be found on our website. You can reach us for questions via genbench@googlegroups.com.

Key deadlines:
August 1, 2023 â€“ Sample data submission deadline
September 1, 2023 â€“ Paper submission deadline
September 15, 2023 â€“ ARR submission deadline

On behalf of the GenBench team,
Dieuwke Hupkes
â€‹Khuyagbaatar Batsuren
Koustuv Sinha
Amirhossein Kazemnejad
Christos Christodoulopoulos
Ryan Cotterell
Elia Brunâ€‹iâ€‹
â€‹Verna Dankers

Encouraging CBT submissions from students

Dear colleague,

We are organising the GenBench workshop (the first workshop on benchmarking generalisation in NLP), co-located with EMNLP 2023.
Generalisation is vital for NLP models, but how do we evaluate that beyond the standard i.i.d testing paradigms?
The first GenBench workshop (1) facilitates discussion on research on generalisation in the NLP community and (2) crowdsources a collaborative generalisation benchmark.

We call for two types of submissions:
(1) Collaborative Benchmarking Task (CBT) submissions that consist of a data/task artefact and a paper describing the submission.
(2) Regular workshop submissions that present papers on the topic of generalisation but are not intended to be included in the collaborative benchmark.

We specifically want to draw your attention to submission type 1. CBT submissions can, for instance, introduce new datasets, new splits of existing datasets, or in-context learning tasks, as long as the submission aims to evaluate non-i.i.d. generalisation of NLP models.
Are your students working on a project that aligns with that? Consider submitting your task/data to GenBench's CBT.
Upon acceptance, we will include it in our platform, which will increase visibility of your students' work in the future.

For both submission types, you can submit archival papers and extended abstracts.
Further information about the workshop and the CBT can be found here: http://genbench.org/workshop, https://genbench.org/cbt/.
You can also reach us for questions via genbench@googlegroups.com.

Key deadlines:
August 1, 2023 â€“ Sample data submission deadline
September 1, 2023 â€“ Paper submission deadline
September 15, 2023 â€“ ARR submission deadline

On behalf of the GenBench team,
Dieuwke Hupkes
â€‹Khuyagbaatar Batsuren
Koustuv Sinha
Amirhossein Kazemnejad
Christos Christodoulopoulos
Ryan Cotterell
Elia Brunâ€‹iâ€‹
â€‹Verna Dankers


Request to circulate CfP




