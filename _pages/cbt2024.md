---
permalink: /cbt/
title: "Collaborative benchmarking task (CBT) -- 2024"
layout: single_wide

excerpt: "Submit to the GenBench Collaborative Benchmarking Task"
header:
  <!-- overlay_image: /assets/images/unsplash-image-1.jpg -->
  overlay_color: "#268"
---

The goal of the second GenBench Collaborative Benchmarking task (CBT), is to generate versions of existing evaluation datasets for LLMs which, given a particular training corpus, have a larger distribution shift from the training corpus than the original test set, or -- in other words -- that evaluate generalisation to a stronger degree.
For this particular challenge, we focus on three training corpora: [C4](https://huggingface.co/datasets/allenai/c4), [RedPAjama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) and [Dolma](https://huggingface.co/datasets/allenai/dolma).
All three corpora are publicly available, and their statistics can be accessed via [What's in My Big Data](https://github.com/allenai/wimbd).
We will focus on four popular evaluation datasets: MMLU, HumanEval, GSM8K and SiQA.

Specifically, submitters to the CBT are asked to design a way to assess distribution shift for one or more of these evaluation datasets, given particular features of the training corpus, and then generate one or more versions of the dataset that have a larger distribution shift according to this method.  
Newly generated sets do not have to have the same size as the original test set, but should have at least 200 examples.
Practically speaking, CBT submissions consist of:
* The data/task artefact, submitted through <https://github.com/GenBench/genbench_cbt>
* A paper describing the dataset and its method of construction, submitted through <https://openreview.net/group?id=GenBench.org/2024/Workshop#tab-recent-activity>

We accept submissions that consider only one pretraining dataset and evaluation dataset, but encourage submitters to apply their suggested protocols to both pretraining datasets.
We also suggest that submitters include model results for models trained on these datasets, [T5](https://huggingface.co/docs/transformers/en/model_doc/t5), [Llama1](https://huggingface.co/docs/transformers/en/model_doc/llama) and [RedPajama-INCITE](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1) and [Olmo](https://huggingface.co/allenai/OLMo-7B), for C4, RedPAjama and Dolma, respectively. 
Given enough high-quality submissions, we aim to write a paper with the combined results, to which submitters can be co-authors, if they wish so.

The paper will be reviewed via a regular paper review pipeline and included in the proceedings of the GenBench workshop. 
Tasks corresponding to accepted papers will be merged into the GenBench repository.

## Important dates (tentative)
- August 15, 2024 -- Paper submission deadline
- September 20, 2024 -- Notification deadline
- October 4, 2024 -- Camera ready deadline
- November 15 or 16, 2024 -- Workshop

## Submit a task to the collaborative benchmark

As described above, CBT submissions consist of a data/task artefact and an accompanying paper.
The data/task artefacts are submitted via a pull request on the [GenBenchCBT github repository](https://github.com/GenBench/genbench_cbt), whereas the papers are submitted via openreview.


### CBT papers

CBT submissions to the Genbench CBT github repository should be accompanied by (anonymous) papers, that can be submitted via the paper submission page of the GenBench workshop.
In the submission form, please indicate that your paper describes a CBT submission, and add the number of your PR in the github repository.
The paper should motivate the development of your proposed task or test, describe the method you used to create the data, present an evaluation of some models of your choosing and contain a [GenBench evaluation card](https://genbench.org/eval_cards/) describing its values on the GenBench generalisation taxonomy.
You can submit both an archival paper or an extended abstract.
Reviewers will be instructed to assess your submission similarly to regular paper submissions, and acceptance of your task is based on the paper, provided that your artefact technically fits within the framework.
Tasks from accepted papers will be merged into the GenBench repository, and the paper will be included in the proceedings of the GenBench workshop. 

## FAQ
#### What is generalisation?
The answer to the question "what is generalisation" likely differs per person, and one of the goals of this year's CBT is to get more actionable protocols to generate datasets that evaluate different forms of generalisation.
We are looking forward to seeing your submissions!

#### Are there any minimal requirements for CBT papers?
CBT submissions should follow the EMNLP 2024 template.
Other than that, there are no strict requirements, though ew do ask that you add a [GenBench evaluation card](https://genbench.org/eval_cards/) to your work to indicate what kind o f generalisation you focus on.
