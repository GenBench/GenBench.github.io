---
permalink: /workshop_programme/
title: "Workshop programme"
toc: false
author_profile: false
layout: single_wide
toc_sticky: true
excerpt: "The programme of the second workshop on generalisation (benchmarking) in NLP"
header:
  overlay_color: "#268"
---

# Programme

Note that all time slots listed below are in Eastern Standard Time (UTC-5) and that all sessions take place in the Brickell room. The programme is tentative at the moment!

## Morning programme

### <span style="color:grey">09:00-09:15 AM —</span> Opening remarks
### <span style="color:grey">09:15-10:00 AM —</span> Keynote 1, by Najoung Kim

[Najoung Kim Speaker](/img/speakers/najoung.png){:width="150px"}

<!---
> !
> <b>Title:</b> [A sanity check on emergent properties](/assets/workshop2023_slides/rogers_genbench2023.pdf)
> 
> <b>Abstract:</b> ...
--> 

### <span style="color:grey">10:00-10:30 AM —</span> Oral presentations

- <b><span style="color:grey">14:45-15:00 PM — </span> Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun pairs, but don’t mimic the full human distribution</b><br>
Hayley Ross, Kathryn Davidson, Najoung Kim

- <b><span style="color:grey">15:00-15:15 PM — </span> Investigating the Generalizability of Pretrained Language Models across Multiple Dimensions: A Case Study of NLI and MRC</b><br>
Ritam Dutt, Sagnik Ray Choudhury, Varun Venkat Rao, Carolyn Rose, V.G. Vinod Vydiswaran

### <span style="color:grey">10:30-11:00 AM —</span> Coffee break

### <span style="color:grey">11:00-11:45 AM —</span> Keynote 2

### <span style="color:grey">11:45-12:30 AM —</span> Spotlight presentations

- <b>MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks</b><br>
Mirelle Candida Bueno, Roberto Lotufo, Rodrigo Frassetto Nogueira

- <b>OmniDialog: A Multimodal Benchmark for Generalization Across Text, Visual, and Audio Modalities</b><br>
Anton Razzhigaev, Maxim Kurkin, Elizaveta Goncharova, Irina Abdullaeva, Anastasia Lysenko, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov

- <b>MultiPragEval: Multilingual Pragmatic Evaluation of Large Language Models</b><br>
Dojun Park, Jiwoo Lee, Seohyun Park, Hyeyun Jeong, Youngeun Koo, Soonha Hwang, Seonwoo Park, Sungeun Lee

- <b>The SlayQA benchmark of social reasoning: testing gender-inclusive generalization with neopronouns</b><br>
Bastian Bunzeck, Sina Zarrieß

- <b>MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models</b><br>
Wentian Wang, Sarthak Jain, Paul Kantor, Jacob Feldman, Lazaros Gallos, Hao Wang

## <span style="color:grey"> 12:30-1:45 PM —</span> Lunch break


## Afternoon programme

### <span style="color:grey">1:45-3:00 PM —</span> Poster session
<details>
<summary>Click to toggle an overview of the posters that will be presented in this session.</summary>
<ul>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks</b> <br>
      Mirelle Candida Bueno, Roberto Lotufo, Rodrigo Frassetto Nogueira
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>OmniDialog: A Multimodal Benchmark for Generalization Across Text, Visual, and Audio Modalities</b> <br>
      Anton Razzhigaev, Maxim Kurkin, Elizaveta Goncharova, Irina Abdullaeva, Anastasia Lysenko, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>MultiPragEval: Multilingual Pragmatic Evaluation of Large Language Models</b> <br>
      Dojun Park, Jiwoo Lee, Seohyun Park, Hyeyun Jeong, Youngeun Koo, Soonha Hwang, Seonwoo Park, Sungeun Lee
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>From Language to Pixels: Task Recognition and Task Learning in LLMs</b> <br>
      Janek Falkenstein, Carolin M. Schuster, Alexander H Berger, Georg Groh
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>Automated test generation to evaluate tool-augmented LLMs as conversational AI agents</b> <br>
      Samuel Arcadinho, David Oliveira Aparicio, Mariana S. C. Almeida
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>Beyond the Numbers: Transparency in Relation Extraction Benchmark Creation and Leaderboards</b> <br>
      Varvara Arzt, Allan Hanbury
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>CHIE: Generative MRC Evaluation for in-context QA with Correctness, Helpfulness, Irrelevancy, and Extraneousness Aspects</b> <br>
      Wannaphong Phatthiyaphaibun, Surapon Nonesung, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Jitkapat Sawatphol, Ekapol Chuangsuwanich, Sarana Nutanong
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>Evaluating the fairness of task-adaptive pretraining on unlabeled test data before few-shot text classification</b> <br>
      Kush Dubey
  </li>
  <li>
      <span style="color:#ffffff; background-color: #ab438a; border-radius:4px; padding:3px">GenBench</span> <b>Towards a new Benchmark for Emotion Detection in NLP: A Unifying Framework of Recent Corpora</b> <br>
      Anna Koufakou, Elijah Nieves, John Peller
  </li>  
  <li>
      <span style="color:#ffffff; background-color: #74849c; border-radius:4px; padding:3px">GenBench CBT</span> <b>The SlayQA benchmark of social reasoning: testing gender-inclusive generalization with neopronouns</b> <br>
      Bastian Bunzeck, Sina Zarrieß
  </li>
  <li>
      <span style="color:#ffffff; background-color: #74849c; border-radius:4px; padding:3px">GenBench CBT</span> <b>MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models</b> <br>
      Wentian Wang, Sarthak Jain, Paul Kantor, Jacob Feldman, Lazaros Gallos, Hao Wang
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners</b> <br>
      Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J Su, Camillo Jose Taylor, Dan Roth
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>Cross-Domain Question Generation: A Comparative Study</b> <br>
      Niloufar Beyranvand, Aijun An, Heidar Davoudi
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>The Relationship Between Compositional Generalization and Misinformation in Emergent Communication</b> <br>
      Heeyoung Lee
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>NeSyCoCo: A Neuro-Symbolic Concept Composer for Compositional Generalization</b> <br>
      Danial Kamali, Elham Barezi, Parisa Kordjamshidi
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>Benchmarking Foundation Models on Exceptional Cases: Dataset Creation and Validation</b> <br>
      Suho Kang, Jungyang Park, Joonseo Ha, SoMin Kim, JinHyeong Kim, Subeen Park, Kyungwoo Song
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>LUCY: Linking Uncertainty and ConsistencY of Large Language Models for Question Answering</b> <br>
      Urja Khurana, Lea Krause
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation?</b> <br>
      Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>Towards Dynamic and Realistic Evaluation of Multi-modal Large Language Model</b> <br>
      Huiqi Zou, Yijiang Li, Ziang Xiao
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0ccfbb; border-radius:4px; padding:3px">GenBench Non-archival</span> <b>Leveraging Isomorphisms to facilitate Zero-Shot KBQA Generalization</b> <br>
      Ritam Dutt, Dongfang Ling, Yu Gu, Carolyn Rose
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0b7ef6; border-radius:4px; padding:3px">Findings</span> <b>Measuring the Robustness of NLP Models to Domain Shifts</b> <br>
      Nitay Calderon, Naveh Porat, Eyal Ben-David, Alexander Chapanin, Zorik Gekhman, Nadav Oved, Vitaly Shalumov, Roi Reichart
  </li>
  <li>
      <span style="color:#ffffff; background-color: #0b7ef6; border-radius:4px; padding:3px">Findings</span> <b>Reconfidencing LLM Uncertainty from the Grouping Loss Perspective</b> <br>
      Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Gaël Varoquaux
  </li>
</ul>
</details> 

### <span style="color:grey">3:00-3:45 PM —</span> Keynote 3, by Sameer Singh
### <span style="color:grey">3:45-4:00 PM —</span> Coffee break
### <span style="color:grey">4:00-4:30 PM —</span> Panel
### <span style="color:grey">4:30-4:45 PM —</span> Closing remarks and best paper award
