---
permalink: /workshop_programme/
title: "GenBench workshop"
toc: false
author_profile: false
layout: single_wide
toc_sticky: true
excerpt: "The programme of the first workshop on (benchmarking) generalisation in NLP"
header:
  overlay_color: "#268"
---

# Programme

Note that all time slots listed below are in Singapore Standard Time (GMT+8), and that all activities take place in the Central Ballroom 3.

## Morning programme

### <span style="color:grey">9-9:15 AM —</span> Opening remarks
### <span style="color:grey">9:15-10 AM —</span> Keynote 1, by Tatsunori Hashimoto
![Tatsunori Hashimoto Speaker](/img/speakers/thashim.jpg){:width="150px"}
> <b>Title:</b> Understanding generalization for instruction following and black-box language models
> 
> <b>Abstract:</b> Instruction following language models have shown a remarkable ability to perform a wide range of tasks with little to no additional training data. Do these abilities come from a revolution in pre-training and instruction-following, or are there other more mundane explanations for how these models work? In this talk, I will discuss our efforts to answer these questions by replicating instruction-following models that generalize across tasks, studying the consistency of these models across different task formats, and building tests for benchmark contamination in pretraining.


### <span style="color:grey">10-11:15 AM —</span> Poster session 1
<details>
<summary>Click to toggle an overview of the posters that will be presented in this session.</summary>
<ul>
  <li>This is a test. The titles and authors will be inserted when the selection is complete.</li>
</ul>
</details>

### <span style="color:grey">10:30-11 AM —</span> Plenary coffee break
### <span style="color:grey">11:15 AM-12:00 PM —</span>  Keynote 2, by Anna Rogers
![Anna Rogers Speaker](/img/speakers/anna.jpg){:width="150px"}

> <b>Title:</b> A sanity check on emergent properties
> 
> <b>Abstract:</b> One of the frequent points in the mainstream narrative about large language models is that they have "emergent properties" (sometimes even dangerous enough to be considered existential risk to mankind). However, there is much disagreement about even the very definition of such properties. If they are understood as a kind of generalization beyond training data - as something that a model does without being explicitly trained for it - I argue that we have not in fact established the existence of any such properties, and at the moment we do not even have the methodology for doing so.


### <span style="color:grey">12:00-12:30 PM —</span> CBT spotlights
- <b>Title:</b> GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding<br>
<b>Authors:</b> Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Ansgar Scherp

- <b>Title:</b> Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study<br>
<b>Authors:</b> Maike Züfle, Verna Dankers, Ivan Titov

- <b>Title:</b> On using distribution-based compositionality assessment to evaluate compositional generalisation in machine translation<br>
<b>Authors:</b> Anssi Moisio, Mathias Creutz, Mikko Kurimo

- <b>Title:</b> Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models<br>
<b>Authors:</b> Jirui Qi, Raquel Fernández, Arianna Bisazza

- <b>Title:</b> Fighting Bias with Bias: Promoting Model Robustness by Amplifying Dataset Biases<br>
<b>Authors:</b> Yuval Reif, Roy Schwartz

## <span style="color:grey"> 12:30-2 PM —</span> Lunch break

## Afternoon programme
### <span style="color:grey">2PM-2:45PM —</span> Keynote 3, by Adina Williams
![Adina Williams Speaker](/img/speakers/adina.jpg){:width="150px"}
<b>Title:</b> Evaluation after the LLM boom: frustrations, fallacies, and the future


### <span style="color:grey">2:45PM-3:30PM —</span> Oral presentations

- <b>Title:</b> Evaluating Neural Language Models as Cognitive Models of Language Acquisition<br>
<b>Authors:</b> Hector Javier Vazquez Martinez, Annika Lea Heuser, Charles Yang, Jordan Kodner

- <b>Title:</b> Understanding Code Semantics: An Evaluation of Transformer Models in Summarization<br>
<b>Authors:</b> Debanjan Mondal, Abhilasha Lodha, Ankita Sahoo, Beena Kumari

- <b>Title:</b> Cross-Lingual Data Augmentation For Thai Question-Answering<br>
<b>Authors:</b> Parinthapat Pengpun, Can Udomcharoenchaikit, Weerayut Buaphet, Peerat Limkonchotiwat


### <span style="color:grey">3:30PM-4PM —</span> Plenary coffee break
### <span style="color:grey">4:00PM-5PM —</span> Poster session 2 (hybrid)
<details>
<summary>Click to toggle an overview of the posters that will be presented in this session.</summary>
<ul>
  <li>This is a test. The titles and authors will be inserted when the selection is complete.</li>
</ul>
</details>

### <span style="color:grey">5PM-5:30PM —</span> Pannel
### <span style="color:grey">5:30PM-5:45PM —</span> Closing remarks and best paper award

